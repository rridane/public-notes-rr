:author-url: https://github.com/rridane
:author: rridane
:source-highlighter: rouge
:hardbreaks:
:table-caption!:
:toc: left

= Deep dive in kubernetes networking

Cette fiche a pour vocation de faire un deep dive √† travers les cni, le r√©seau de kubernetes. La strat√©gie adopt√©e pour cela, est de monter petit √† petit un r√©seau √©quivalent √† celui de kubernetes en traversant petit √† petit les concepts, par difficult√© croissante.

== Rappels: Routage, masque et ARP

=== Qui r√©pond √† une requ√™te ARP ?

- Seule la machine qui **poss√®de l‚Äôadresse IP** r√©pondra √† une requ√™te ARP de type :
`Who has 192.168.100.2 ?`
- Le masque `/24` ne veut pas dire ‚Äúje poss√®de toutes les IP du r√©seau‚Äù.
-> La machine ne r√©pond qu‚Äôaux ARP correspondant √† **ses propres IP**.
-> Mais elle consid√®re toutes les adresses du `/24` comme *on-link* (donc √©ligibles √† une r√©solution ARP).

=== ARP local vs pas d‚ÄôARP hors r√©seau
- Exemple : interface avec `192.168.100.2/24`.

Cas local :
[source,bash]
----
ping 192.168.100.3
# -> 192.168.100.3 est dans 192.168.100.0/24
# -> r√©solution ARP "Who has 192.168.100.3 ?"
----

Cas hors r√©seau :
[source,bash]
----
ping 10.0.0.1
# -> 10.0.0.1 n‚Äôest pas dans 192.168.100.0/24
# -> pas de requ√™te ARP
# -> recherche dans la table de routage
# -> si une route "default via 192.168.100.1" existe, ARP sera fait pour 192.168.100.1 (la passerelle)
----

=== R√¥le des routes

- Les **routes** indiquent par quel next-hop (passerelle) atteindre une destination.
- Exemple de table de routage classique sur un poste :

[source,bash]
----
$ ip route show
default via 192.168.100.1 dev eth0    # route par d√©faut (gateway)
192.168.100.0/24 dev eth0 proto kernel scope link src 192.168.100.2
----
- Ici :
* Tout paquet vers 192.168.100.X ‚Üí direct sur le lien via ARP.
* Tout paquet vers un autre r√©seau ‚Üí envoi √† la passerelle 192.168.100.1.

=== Communication entre deux r√©seaux via un routeur

Supposons deux r√©seaux :
- R√©seau 2 : 192.168.2.0/24
- R√©seau 3 : 192.168.3.0/24

Un poste `192.168.2.10` veut contacter `192.168.3.20` :
1. `192.168.2.10` d√©tecte que `192.168.3.20` n‚Äôest pas local.
2. Il envoie le paquet √† sa passerelle `192.168.2.1` (via ARP pour sa MAC).
3. Le routeur `192.168.2.1` re√ßoit, et a aussi une interface `192.168.3.1`.
4. Il transf√®re le paquet sur le r√©seau 3.
5. ARP est fait pour obtenir la MAC de `192.168.3.20` ‚Üí livraison finale.

=== netfilter ?

Netfilter est le moteur dans le kernel Linux qui g√®re:

* le filtrage des paquets
* le NAT
* la modification des paquets (mangling)
* la connexion suivie (conntrack)

**iptables (ancien)** et **nftables (nouveau)** sont des interfaces en espace utilisateur pour configurer Netfilter.

En pratique, quand on dit "netfilter", on pense "iptables/nftables"

=== sysctl

sysctl est une contraction de syst√®me control. C'est une interface (au sens code ou contrat) qui permets de lire ou de modifier certains param√®tres du kernel linux **pendant qu'il tourne**

sysctl peut contr√¥ler des param√®tres m√©morie, r√©seau, filesystems, s√©curit√© etc.. On peut les voir comme des variables globales du noyau modifiables √† chaud et accessibles en lecture √©criture.

Les fichiers virtuels sont accessibles sous **/proc/sys**, ils contiennent notamment le dossier /proc/net/. Pour plus de d√©tails sur sysctl voir la fiche associ√©e;

[source,bash]
----
‚ù≠ ls /proc/sys

...

# pile r√©seua, c'est ici que l'on param√®tre le r√©seau, et que l'on peut isole la pile r√©seau par un namespace
# c'est le dossier que les conteneur isolent dans leur netns
# net/ipv4/ ‚Üí IPv4 (ex: ip_forward, tcp_fin_timeout).
# net/ipv6/ ‚Üí IPv6.
# net/core/ ‚Üí buffers, files d‚Äôattente, backlog.
# net/bridge/ ‚Üí options bridge (souvent pour Kubernetes/CNI).
# net/netfilter/ ‚Üí interaction avec Netfilter (conntrack, etc.).
net/
----

.manipuler les param√®tres r√©seaux avec sysctl
[source,bash]
----
# /proc/sys/net
# R√©seau

# Lire
sysctl net.ipv4.ip_forward
cat /proc/sys/net/ipv4/ip_forward

# √âcrire
sysctl -w net.ipv4.ip_forward=1
echo 1 > /proc/sys/net/ipv4/ip_forward
----

=== Qu'est ce qui fait une machine d'un point de vue r√©seau

Ce qui fait une machine d'un point de vue r√©seau c'est :

* une table de routage ind√©pendante
* une table ARP ind√©pendante
* des interfaces visibles ind√©pendantes (lo, veth, dummy..)
* des tables de r√®gles / netfilter ind√©pendantes
* sysctl r√©seau ind√©pendant (dans /proc/sys/net isol√© par namespace)

Ces √©l√©ments forment une pile r√©seau compl√®te.

Ainsi, lorsque l'on cr√©√© un namespace, et que l'on execute un process sur ce namespace, on donne √† ce process sa propre pile TCP/IP. D'un point de vue r√©seau cela revient √† lui fournir son propre kernel, et donc √† le mettre sur une machine √† part.

D'un point vue code, le kernel maintient une instance s√©par√©e des structures de donn√©es r√©seau pour chaque netns (**struct netns**)

=== Autres informations importantes

Le **routing L3 pur** dans Linux est *agnostique de l‚Äôinterface d‚Äôentr√©e*.
Autrement dit : peu importe si le paquet est arriv√© par `cni0`, `ens160` ou une veth, le noyau ne se pose m√™me pas la question.
Il regarde uniquement **l‚Äôadresse IP de destination** ‚Üí fait un **lookup dans la FIB** (table de routage) ‚Üí et choisit une **interface de sortie**.

Mais il existe **4 grandes exceptions** o√π l‚Äôinterface d‚Äôentr√©e ou de sortie (`skb->dev`) a un r√¥le explicite et direct :

* **rp_filter** ‚Üí s√©curit√© anti-spoofing : compare interface d‚Äôentr√©e vs interface de sortie attendue pour la source.
* **Policy routing** ‚Üí permet de choisir une table de routage alternative en fonction de l‚Äôinterface, de l‚Äôadresse source, ou encore d‚Äôune marque (`fwmark`).
* **Netfilter (iptables/nftables)** ‚Üí firewall/NAT/mangle, avec des r√®gles explicites qui peuvent matcher sur `-i` (interface d‚Äôentr√©e) ou `-o` (interface de sortie).
* **Traffic Control (tc)** ‚Üí gestion des files et du scheduling par interface, chaque interface ayant sa propre qdisc (sa file d‚Äôattente de paquets).

üëâ On peut voir ces exceptions comme des ‚Äúpoints de personnalisation‚Äù o√π Linux sort de son comportement par d√©faut pour prendre en compte le contexte de l‚Äôinterface.

=== Par namespace

Chaque **network namespace** dispose de sa **pile r√©seau compl√®te et s√©par√©e**.

Cela inclut :

* une **table de routage ind√©pendante**,
* ses propres r√®gles **policy routing** (`ip rule`),
* ses param√®tres sysctl (donc son propre r√©glage `rp_filter`),
* son instance **Netfilter** (iptables/nftables s√©par√©es),
* ses qdisc et filtres **tc** attach√©s aux interfaces de ce namespace.

üëâ En pratique :
- Deux namespaces = **deux stacks TCP/IP totalement isol√©es**.
- Chacun applique ses propres ‚Äúexceptions‚Äù (rp_filter, ip rule, iptables, tc).
- C‚Äôest exactement ce qui rend les conteneurs possibles : chaque conteneur croit √™tre une ‚Äúmachine ind√©pendante‚Äù, avec ses propres r√®gles de firewall, de routage et de QoS, m√™me si tout tourne sur le m√™me noyau Linux.

=== En quoi ces informations suppl√©mentaires sont importantes

Nous allons par la suite simuler une stack kubernetes √† l'aide des namespace. Il est essentiel d'avoir en t√™te que les interfaces au sein d'un bridge, ou d'un namespace partageront la m√™me logique, et que peu importe par ou arrive le traffic, le traitement sera similaire. C'est la table de routage qui fait foi.

== R√©seau de bases √† l'aide de linux

=== Les bases : ip command & basic network

[source,bash]
----
apt install iproute2
----

[source,bash]
----
# Structure de la commande

ip [OPTIONS GLOBALES] OBJET COMMANDE [OPTIONS DE LA COMMANDE]
----

[source,bash]
----
# verticalement

ip

# options globales
-n
-4
-6
-br

#object : Ce que l'on veut manipuler
link # interfaces (Ethernet, loopback, veth, ...)
addr # adresses IP
route # routes
netns # namespaces r√©seau
rule
neigh
maddr
mroute

#commande: Commande sur l'objet
add
del
show
set
----

[source,bash]
----
# Exemple avanc√©

ip link add # Ajoutons une interface d'un certain type

# options
type veth # une nouvelle interface
peer name veth-sat # nom de l'autre interface √† l'extr√©mit√©

# autres options
type dummy # qui ne transmet rien (utile pour assigner une ip sans device r√©√©l)
type bridge # bridge logiciel (comme un switch)
type bond # agr√©gat de cartes (LACP)
type vlan id 100 link eth0 # inteface VLAN sur eth0
----

Nous verrons un exemple par la suite, mais c r√©er un bridge revient √† cr√©er un switch, on peut lui assigner une ip, cela signifie lui assigner une ip d'administration. Mais sont objectif principal est de se comporter en switch.

[source,bash]
----
ip link show # montre les informations concernant les interfaces
ip -n fw link show # montre les interfaces concernant le namespace fw (ici -n fw est une option globale)
ip addr add 10.10.10.1/24 dev veth-fw # donne l'adresse 10.10.10.1/24 √† l'interface veth-fw. Elle sait d√©sormais qu'elle aura la responsabilit√© de r√©pondre aux requ√™tes arp pour ce r√©seau. Les autres interfaces du switch associeront √©galement sont adresse mac √† ce r√©seau, routant ainsi le traffic.
----

[source,bash]
----
# Quand on ajoute une adresse avec /24, le noyau ajoute une route automatique :
# Exemple :
#   192.168.100.0/24 dev br0 proto kernel scope link src 192.168.100.1
#
# Cette route dit : "Tout le r√©seau /24 est atteignable directement via br0"
ip route show
----

[source,bash]
----
# Deux mani√®res d'agir dans un namespace :

# 1. Avec l'option globale -n :
#    -> on utilise ip pour ex√©cuter une sous-commande (link, addr, route...)
ip -n fw addr show

# 2. Avec "ip netns exec" :
#    -> on lance n'importe quel programme utilisateur dans le namespace
ip netns exec fw ping 192.168.100.1
----

[source,bash]
----
# Cr√©er deux interfaces dummy (simulent des cartes physiques)
ip link add dummy1 type dummy
ip link add dummy2 type dummy

# Mettre les interfaces UP
ip link set dummy1 up
ip link set dummy2 up

# Cr√©er un bridge (switch logiciel)
ip link add br0 type bridge

# Mettre le bridge UP
ip link set br0 up

# Relier les dummy au bridge (comme si on les branchait sur le switch)
ip link set dummy1 master br0
ip link set dummy2 master br0

# Assigner une IP au bridge (il devient un n≈ìud/gateway)
ip addr add 192.168.100.1/24 dev br0

# Donner des IP aux dummy (comme si elles appartenaient √† deux machines du r√©seau)
ip addr add 192.168.100.2/24 dev dummy1
ip addr add 192.168.100.3/24 dev dummy2

# V√©rifier la table ARP
ip neigh show

# Tester la connectivit√© entre les interfaces (via le bridge)
ping -c 3 -I dummy1 192.168.100.3
ping -c 3 -I dummy2 192.168.100.2

# Tester la connectivit√© vers le bridge lui-m√™me (son IP d‚Äôadmin)
ping -c 3 -I dummy1 192.168.100.1
ping -c 3 -I dummy2 192.168.100.1
----

[plantuml,netfilter-flow,png]
----
@startuml
skinparam dpi 150
skinparam componentStyle rectangle
skinparam rectangle {
BackgroundColor #f5f5f5
BorderColor #888
RoundCorner 10
}
skinparam note BackgroundColor #ffffcc
skinparam ArrowColor #555

title Topologie L2 : bridge br0 avec deux h√¥tes (dummy1 & dummy2)

rectangle "root netns" as ROOT {
rectangle "br0\n(type: bridge)\nIP: 192.168.100.1/24 (optionnel)" as BR0

  rectangle "Port: dummy1\n(type: dummy)\nIP: 192.168.100.2/24\nMAC: 02:aa:aa:aa:aa:01" as D1
  rectangle "Port: dummy2\n(type: dummy)\nIP: 192.168.100.3/24\nMAC: 02:bb:bb:bb:bb:02" as D2
}

D1 -[#gray;thickness=2]- BR0
D2 -[#gray;thickness=2]- BR0

note right of BR0
br0 agit comme un switch L2 :
- Apprend les MAC
- Commutation de trames
- Optionnellement une IP d'admin
end note

legend left
__L√©gende__
- br0 : switch logiciel (bridge)
- dummy1/dummy2 : h√¥tes/ports du bridge
- M√™me /24 ‚áí "on-link" ‚áí ARP possible
endlegend
@enduml
----

L'exemple pr√©c√©dent pourrait pr√™ter √† confusion, la seule vocation des dummy port est de recevoir du traffic. Ce qui a √©t√© fait plus haut c'est de brancher deux ports ethernet sur la m√™me machine via un bridge. Ainsi, factuellement, et en l'absence d'isolation, notamment des param√®tres sysctl, la seule vocation de ces deux ports est de recevoir du traffic sur deux ips diff√©rentes du m√™me r√©seau. (si l'on oublie le fait qu'elles soient des dummy interfaces).

=== Isoler les r√©seaux avec les namespaces

==== quelques fonctions pour les sections suivantes

[source,bash]
----
#!/bin/bash

# $1 interface 1
# $2 interface 2
function create_linked_interfaces() {
  ip link add $1 type veth peer name $2
}

# $1 bridge name
function create_bridge() {
  ip link add $1 type bridge
  ip link set $1 up
}

# $1 namespace
function create_namespace() {
  ip netns add $1
}

# $1 interface
# $2 namespace
function move_interface_to_namespace() {
  ip link set $1 netns $2
}

# $1 interface
# $2 bridge name
function move_interface_to_bridge() {
  ip link set $1 master $2
}

# $1 interface
# $2 ns
# $3 ip
function set_ip_and_mask_to_interface_in_namespace() {
  ip -n $2 addr add $3 dev $1
  ip -n $2 link set $1 up
}

function set_ip_and_mask_to_interface_in_root() {
  ip addr add $2 dev $1
  ip link set $1 up
}

# $1 namespace
function activate_loopback_on_namespace() {
  ip -n $1 link set lo up
}
----

=== Simuler un r√©seau kubernetes sur un seul noeud

[source,bash]
----
#!/bin/bash
create_namespace "pod0"
create_linked_interfaces "eth0-pod0" "to-pod0"
move_interface_to_namespace "eth0-pod0" "pod0"

create_namespace "pod1"
create_linked_interfaces "eth0-pod1" "to-pod1"
move_interface_to_namespace "eth0-pod1" "pod1"

create_bridge "cni0"
move_interface_to_bridge "to-pod0" "cni0"
move_interface_to_bridge "to-pod1" "cni0"

set_ip_and_mask_to_interface_in_namespace "eth0-pod0" "pod0" "192.168.1.2/24"
set_ip_and_mask_to_interface_in_namespace "eth0-pod1" "pod1" "192.168.1.3/24"

activate_loopback_on_namespace "pod0"
activate_loopback_on_namespace "pod1"

set_ip_and_mask_to_interface_in_root "cni0" "192.168.1.1/24"
----

Dans le code ci-dessus:

* chaque pod a son netns isol√© avec une ip unique
* tous les pods du m√™me noeud sont sur un bridge commun (cni0)
* le host (root netns) joue le r√¥le de gateway

C'est ainsi que kubernetes construit le r√©seau sur une seule machine. Dans kubernetes, chaque machine dispose de sa cni0 (d√®s qu'elle a un pod). Sa cni0 se comporte comme un switch L3, capable de r√©cup√©rer du traffic sur son sous r√©seau d√©di√©. Ainsi, tous les pods sur une m√™me machine sont dans un m√™me sous r√©seau. (sous r√©seau du pod subnet global associ√© au cni).

Ainsi il ne reste qu'√† lier les machines entre elles pour simuler une cni. C'est ici que le coeur de chaque cni va prendre sens.

=== Multi nodes

A l'aide des namespaces commen√ßons par simuler deux noeuds (node1 et node2), et par simuler la cni0, et deux pods sur chaque noeud. Ainsi, nous aurons les bases de deux noeuds, contenant des pods, mais sans cni, donc incapables de communiquer d'un node √† l'autre (ou plus pr√©cis√©ment d'un pod sur un node √† un autre pod sur un autre node)

Avant toute chose nous ajustons / cr√©ons quelques m√©thodes
[source,bash]
----
# $1 interface
# $2 namespace
# $3 bridge
function move_interface_to_namespace_bridge() {
  ip link set "$1" netns "$2"
  ip -n "$2" link set "$1" master "$3"
  ip -n "$2" link set "$1" up
}

# $1 namespace
# $2 bridge_name
function create_bridge_on_namespace() {
  ip -n $1 link add $2 type bridge
  ip -n $1 link set $2 up
}

# $1 namespace
# $2 interface name
function create_interface_on_namespace() {
  ip -n $1 link add $2 type dummy
  ip -n $1 link set $2 up
}
----

[source,bash]
----
# Cr√©ation des noeuds
create_namespace "node1"
create_namespace "node2"

# cr√©ation des cni0 par noeuds
create_bridge_on_namespace "node1" "cni0"
create_bridge_on_namespace "node2" "cni0"

# cr√©ation des pods
create_namespace "node1-pod1"
create_namespace "node1-pod2"
create_namespace "node2-pod1"
create_namespace "node2-pod2"

# Cr√©ation des paire d'interfaces pour chaque pod
create_linked_interfaces "eth0-n1pod1" "veth-to-n1pod1"
create_linked_interfaces "eth0-n1pod2" "veth-to-n1pod2"
create_linked_interfaces "eth0-n2pod1" "veth-to-n2pod1"
create_linked_interfaces "eth0-n2pod2" "veth-to-n2pod2"

# Rattachement de eth0 √† chaque pod
move_interface_to_namespace "eth0-n1pod1" "node1-pod1"
move_interface_to_namespace "eth0-n1pod2" "node1-pod2"
move_interface_to_namespace "eth0-n2pod1" "node2-pod1"
move_interface_to_namespace "eth0-n2pod2" "node2-pod2"

# Rattachement des veth aux cni0 bridges de chaque noeud
move_interface_to_namespace_bridge "veth-to-n1pod1" "node1" "cni0"
move_interface_to_namespace_bridge "veth-to-n1pod2" "node1" "cni0"
move_interface_to_namespace_bridge "veth-to-n2pod1" "node2" "cni0"
move_interface_to_namespace_bridge "veth-to-n2pod2" "node2" "cni0"

# Ici nous avons nos pods rattach√©s aux cni0 de chaque noeud par leur eth0
# On continue en cr√©ant les sous r√©seaux

# ip des eth0 des pods

# 10.244.1.0/24 pour le node 1
set_ip_and_mask_to_interface_in_namespace "eth0-n1pod1" "node1-pod1" "10.244.1.2/24"
set_ip_and_mask_to_interface_in_namespace "eth0-n1pod2" "node1-pod2" "10.244.1.3/24"
# 10.244.2.0/24 pour le node 2
set_ip_and_mask_to_interface_in_namespace "eth0-n2pod1" "node2-pod1" "10.244.2.2/24"
set_ip_and_mask_to_interface_in_namespace "eth0-n2pod2" "node2-pod2" "10.244.2.3/24"

# ip des cni0 sur chaque node
set_ip_and_mask_to_interface_in_namespace "cni0" "node1" "10.244.1.1/24"
set_ip_and_mask_to_interface_in_namespace "cni0" "node2" "10.244.2.1/24"

# activation des loopbacks
activate_loopback_on_namespace "node1-pod1"
activate_loopback_on_namespace "node1-pod2"
activate_loopback_on_namespace "node2-pod1"
activate_loopback_on_namespace "node2-pod2"

# activation des loopbacks sur le noeuds
activate_loopback_on_namespace "node1"
activate_loopback_on_namespace "node2"
----

Pour plus de r√©alisme, et pour aider √† la projection, nous donnons une interface ens160 √† chaque noeud, puis nous ajoutons un routeur, par l'interm√©diaire d'un namespace qui active ip_forward.

Nous ajoutons ainsi une fonction pour activer l'ip_forward

[source,bash]
----
# $1 namespace
function enable_ip_forward_on_namespace() {
  ip netns exec $1 sysctl -w net.ipv4.ip_forward=1
}
----

Puis nous montons le r√©seau associ√©

[source,bash]
----
# cr√©ation du routeur
create_namespace "router"
activate_loopback_on_namespace "router"
enable_ip_forward_on_namespace "router"
----

[source,bash]
----
# cr√©ation des paire d'interfaces
create_linked_interfaces "ens160-node1" "veth-to-node1"
create_linked_interfaces "ens160-node2" "veth-to-node2"

# rattachement des interfaces aux noeuds
move_interface_to_namespace "ens160-node1" "node1"
move_interface_to_namespace "ens160-node2" "node2"

# rattachement des interfaces virtuelles aux namespaces du routeur
move_interface_to_namespace "veth-to-node1" "router"
move_interface_to_namespace "veth-to-node2" "router"

# adresses ips des ens160
set_ip_and_mask_to_interface_in_namespace "ens160-node1" "node1" "10.33.18.10/24"
set_ip_and_mask_to_interface_in_namespace "ens160-node2" "node2" "10.33.18.11/24"

# adresse ip des interfaces du routeur
set_ip_and_mask_to_interface_in_namespace "veth-to-node1" "router" "10.33.18.1/24"
set_ip_and_mask_to_interface_in_namespace "veth-to-node2" "router" "10.33.18.2/24"
----

Enfin on configure les routes par d√©faut de tous les pods, pour que n'importe quel traffic passe par la cni0 par eth0:

[source,bash]
----
# $1 namespace
# $2 destination CIDR
# $3 via (next-hop IP)
# $4 interface
function add_route_on_namespace() {
  ip -n $1 route add $2 via $3 dev $4
}
----

[source,bash]
----
# Routes par d√©faut des pods du node1 (gateway = 10.244.1.1)
add_route_on_namespace "node1-pod1" "default" "10.244.1.1" "eth0"
add_route_on_namespace "node1-pod2" "default" "10.244.1.1" "eth0"

# Routes par d√©faut des pods du node2 (gateway = 10.244.2.1)
add_route_on_namespace "node2-pod1" "default" "10.244.2.1" "eth0"
add_route_on_namespace "node2-pod2" "default" "10.244.2.1" "eth0"
----

.Sch√©ma Netfilter : de la carte r√©seau au socket utilisateur
[plantuml,multi-node,png]
----
@startuml
skinparam dpi 150
skinparam componentStyle rectangle
skinparam rectangle {
  BackgroundColor #f5f5f5
  BorderColor #888
  RoundCorner 10
}
skinparam note {
  BackgroundColor #ffffcc
}

title Simulation Kubernetes multi-n≈ìuds avec un routeur interm√©diaire

package "node1 (netns)" {
  rectangle "ens160-node1\n10.33.18.10/24" as N1ENS
  rectangle "cni0\n10.244.1.1/24" as N1CNI
  rectangle "pod1-node1\neth0=10.244.1.2" as N1P1
  rectangle "pod2-node1\neth0=10.244.1.3" as N1P2

  N1P1 -[thickness=2]- N1CNI
  N1P2 -[thickness=2]- N1CNI
}

package "node2 (netns)" {
  rectangle "ens160-node2\n10.33.18.11/24" as N2ENS
  rectangle "cni0\n10.244.2.1/24" as N2CNI
  rectangle "pod1-node2\neth0=10.244.2.2" as N2P1
  rectangle "pod2-node2\neth0=10.244.2.3" as N2P2

  N2P1 -[thickness=2]- N2CNI
  N2P2 -[thickness=2]- N2CNI
}

package "router (netns)" {
  rectangle "veth-to-node1\n10.33.18.1/24" as R1
  rectangle "veth-to-node2\n10.33.18.2/24" as R2
}

' Liaisons physiques simul√©es
N1ENS -[thickness=2]- R1
N2ENS -[thickness=2]- R2

note bottom
- Chaque n≈ìud a un PodCIDR distinct (10.244.1.0/24 et 10.244.2.0/24)
- Le namespace "router" joue le r√¥le de routeur L3 entre les n≈ìuds
- ip_forward=1 est activ√© dans "router"
- Inter-pods inter-n≈ìuds : toujours KO tant qu‚Äôil n‚Äôy a pas de routes
end note
@enduml
----

== CNI - D√©finition

Avant de parler de la communication inter-n≈ìuds, il convient ici de distinguer ce qui rel√®ve de Kubernetes, ce qui rel√®ve de la CNI, et ce qui rel√®ve de la configuration r√©seau des administrateurs.

=== Briques administration r√©seau / infra classique

* les n≈ìuds (machines physiques ou VM) et leurs adresses IP
* le routeur et la d√©claration des routes par d√©faut vers ce routeur

C'est ici que s'arr√™te la responsabilit√© de l'administration r√©seau / infra.

=== Briques Kubernetes et CNI

==== Lorsque Kubernetes re√ßoit une requ√™te POST pour cr√©er un Pod

*Kubernetes API Server (Kubernetes pur)*

- L‚Äôobjet Pod est cr√©√© dans etcd via l‚ÄôAPI Server.
- Le Scheduler d√©cide sur quel n≈ìud placer le Pod.
- Le kubelet de ce n≈ìud re√ßoit l‚Äôordre : *¬´ cr√©e-moi ce Pod ¬ª*.
üëâ √Ä ce stade, c‚Äôest purement Kubernetes, rien de r√©seau encore.

*Kubelet (Kubernetes pur)*

- Le kubelet pr√©pare l‚Äôenvironnement d‚Äôex√©cution du Pod :
* cr√©ation d‚Äôun **namespace r√©seau** pour isoler le Pod, (`create_namespace "podX"`)
- Il doit maintenant donner **un r√©seau** au Pod, cr√©er ses interfaces, il d√©l√®gue tout ceci √† la cni.

*Appel au CNI (contrat CNI ‚Üí responsabilit√© du plugin)*

- kubelet invoque le binaire CNI (par d√©faut `/opt/cni/bin/...`) avec la spec `ADD`.
- Il passe en argument :
* le nom du netns du Pod,
* le nom de l‚Äôinterface attendue (`eth0`),
* le PodCIDR du n≈ìud.
- C‚Äôest maintenant au plugin CNI (Flannel, Calico, Cilium‚Ä¶) d‚Äôagir.

*Ce que fait le plugin CNI (r√©seau pur)*

- Cr√©e une *veth pair* (`eth0` c√¥t√© Pod, `vethXXXXX` c√¥t√© n≈ìud) `create_linked_interfaces "eth0-podX" "to-podX"`.
- Place `eth0` dans le netns du Pod. (`move_interface_to_namespace "eth0-podX" "podX"`)
- Connecte `vethXXXXX` au bridge `cni0` (local au n≈ìud)(`move_interface_to_bridge "to-podX" "cni0"`).
- **Attribue une IP au Pod, choisie dans le PodCIDR du n≈ìud.** (set_ip_and_mask_to_interface_in_namespace "eth0-podX" "podX" "10.244.X.Y/24")
- Configure les routes locales pour que le n≈ìud sache joindre le Pod via `cni0`.(`add_route_on_namespace "node1-pod1" "default" "10.244.1.1" "eth0"`)

Nous voyons donc imm√©diatement le parrall√®le avec ce que nous avons fait pr√©c√©demment, et les responsabilit√©s se clarifient.

== Communication inter-n≈ìuds (responsabilit√© CNI selon le plugin)

=== Rappel : √©tat actuel

pod1-node1 (10.244.1.2) peut joindre son cni0 (10.244.1.1).

pod1-node2 (10.244.2.2) peut joindre son cni0 (10.244.2.1).

Mais il n‚Äôexiste pas encore de route sur node1 pour atteindre 10.244.2.0/24, ni sur node2 pour 10.244.1.0/24.
üëâ Le trafic reste bloqu√©.

=== Ce que fait un CNI inter-n≈ìuds

Selon le plugin, plusieurs strat√©gies :

* *Flannel host-gw* : ajoute des routes statiques sur chaque n≈ìud :
** sur node1 ‚Üí pour atteindre 10.244.2.0/24, passer par 10.33.18.2 (IP du node2 sur le r√©seau physique).
** sur node2 ‚Üí pour atteindre 10.244.1.0/24, passer par 10.33.18.1.

* *Flannel vxlan* : encapsule tout trafic inter-n≈ìuds dans du VXLAN, pas besoin de routes explicites (tout transite par une interface vxlan0).

* *Calico (BGP)* : chaque n≈ìud annonce son PodCIDR via BGP ‚Üí les routes apparaissent automatiquement.

* *Cilium (eBPF)* : injecte directement des programmes eBPF pour router (ou rediriger) les paquets Pod ‚Üî Pod.

Voyons dans le d√©tail la suite en ligne de commande pour chaque strat√©gie

=== "Flannel host-gw" minimaliste (statique)

On compl√®te notre maquette Linux avec des routes sur chaque n≈ìud :

[source,bash]
----
# Sur node1 : joindre les Pods de node2 via l‚ÄôIP du routeur c√¥t√© node2
add_route_on_namespace "node1" "10.244.2.0/24" "10.33.18.2" "ens160-node1"
----

[source,bash]
----
# Sur node2 : joindre les Pods de node1 via l‚ÄôIP du routeur c√¥t√© node1
add_route_on_namespace "node2" "10.244.1.0/24" "10.33.18.1" "ens160-node2"
----

==== R√©sultat

* `ping 10.244.2.2` depuis node1-pod1 fonctionne üéâ
* `ping 10.244.1.2` depuis node2-pod1 fonctionne üéâ

On a donc reproduit le comportement d‚Äôun cluster Kubernetes avec Flannel en mode *host-gw* :

* Chaque PodCIDR est rout√© via l‚ÄôIP du n≈ìud qui l‚Äôh√©berge.
* Les Pods ont des routes par d√©faut pointant vers `cni0`.
* Les n≈ìuds eux-m√™mes ont des routes vers les PodCIDR distants.

==== Diagramme r√©capitulatif

[plantuml,cni-flannel-hostgw,png]
----
@startuml
skinparam dpi 150
skinparam rectangle {
  BackgroundColor #f5f5f5
  BorderColor #888
  RoundCorner 10
}

title CNI : Flannel host-gw (routes statiques inter-n≈ìuds)

package "node1 (netns)" {
  rectangle "ens160-node1\n10.33.18.10/24" as N1ENS
  rectangle "cni0\n10.244.1.1/24" as N1CNI
  rectangle "pod1-node1\n10.244.1.2/24" as N1P1
  rectangle "pod2-node1\n10.244.1.3/24" as N1P2

  N1P1 -[thickness=2]- N1CNI
  N1P2 -[thickness=2]- N1CNI
}

package "node2 (netns)" {
  rectangle "ens160-node2\n10.33.18.11/24" as N2ENS
  rectangle "cni0\n10.244.2.1/24" as N2CNI
  rectangle "pod1-node2\n10.244.2.2/24" as N2P1
  rectangle "pod2-node2\n10.244.2.3/24" as N2P2

  N2P1 -[thickness=2]- N2CNI
  N2P2 -[thickness=2]- N2CNI
}

package "router (netns)" {
  rectangle "veth-to-node1\n10.33.18.1/24" as R1
  rectangle "veth-to-node2\n10.33.18.2/24" as R2
}

N1ENS -[thickness=2]- R1
N2ENS -[thickness=2]- R2

note bottom
Pods en 10.244.1.0/24 (node1) et 10.244.2.0/24 (node2)

Chaque Pod ‚Üí d√©faut via cni0

Chaque Node ‚Üí route statique vers le PodCIDR de l‚Äôautre

Router = simple L3 forwarder
end note
@enduml
----

=== Flannel - mode VXLAN

Le mode VXLAN est le mode par d√©faut de Flannel.
Il cr√©e un overlay r√©seau au-dessus de l‚Äôunderlay (r√©seau physique des n≈ìuds).

Chaque Pod conserve une IP unique dans le cluster (ex: `10.244.X.Y`), mais les paquets sont encapsul√©s dans UDP pour circuler entre les n≈ìuds.

==== Cr√©ation de l‚Äôinterface VXLAN

Commen√ßons par cr√©er quelques m√©thodes suppl√©mentaires:

[source,bash]
----
# $1 namespace
# $2 vxlan ifname
# $3 VNI (ex: 4096)
# $4 underlay dev (ex: ens160-node1)
# $5 dstport (ex: 4789)
function create_vxlan_on_namespace() {
  ip -n $1 link add $2 type vxlan id $3 dev $4 dstport $5
  ip -n $1 link set $2 up
}


# $1 namespace
# $2 dev (ex: vxlan0)
# $3 dst underlay IP (ex: 10.33.18.11)
function add_fdb_on_namespace() {
  ip -n "$1" bridge fdb add 00:00:00:00:00:00 dev "$2" dst "$3"
}
----

[NOTE]
--

En appelant ceci:
[source,bash]
----
create_vxlan_on_namespace "node1" "vxlan0" "4096" "ens160-node1" "4789"
----

Nous faisons:
[source,bash]
----
ip -n node1 link add vxlan0 type vxlan id 4096 dev ens160-node1 dstport 4789
ip -n node1 link set vxlan0 up
----

C'est une syntaxe que nous n'avons pas encore vu, c'est ainsi que l'on cr√©√© une interface vxlan, en lui sp√©cifiant l'interface r√©elle √† qui transf√©rer les paquets une fois encapsul√©s en udp.
--

Cr√©ons ensuite le datapath VXLAN sur chaque n≈ìud, puis la ¬´ colle ¬ª L3‚ÜîL2 :

[source,bash]
----
# $1 namespace
# $2 ifname
# $3 bridge name
function attach_iface_to_bridge_in_namespace() {
  ip -n "$1" link set "$2" master "$3"
  ip -n "$1" link set "$2" up
}
----


[source,bash]
----
# --- VXLAN sur node1 ---
create_vxlan_on_namespace "node1" "vxlan0" "4096" "ens160-node1" "4789"
attach_iface_to_bridge_in_namespace "node1" "vxlan0" "cni0"

# --- VXLAN sur node2 ---
create_vxlan_on_namespace "node2" "vxlan0" "4096" "ens160-node2" "4789"
attach_iface_to_bridge_in_namespace "node2" "vxlan0" "cni0"

# --- FDB : vers quel n≈ìud underlay flooder/encapsuler ---
add_fdb_on_namespace "node1" "vxlan0" "10.33.18.11"
add_fdb_on_namespace "node2" "vxlan0" "10.33.18.10"

# --- Routes L3 PodCIDR distants ‚Üí cni0 (overlay) ---
# (r√©utilise ta fonction existante add_route_on_namespace si tu pr√©f√®res du "via",
#  mais ici on veut explicitement "dev cni0" pour pousser dans le bridge)
ip -n node1 route add 10.244.2.0/24 dev cni0
ip -n node2 route add 10.244.1.0/24 dev cni0
----

Voici ce que nous avons fait:

* Nous avons cr√©√© une interface vxlan rattach√©e au bridge cni0, qui encapsulera tout paquet re√ßu en udp pour les transf√©rer √† l'interface machine ens160-node1 (ou 2)
* Nous avons ajout√© une entr√©e √† la forward table (fdb) du node1 et node2. Elle signifie, transf√®re tout paquet dont l'adresse mac de destination

Sur chaque n≈ìud, Flannel cr√©e une interface VXLAN (`flannel.1` ou `vxlan0`) .

* `id 4096` : le **VNI** (VXLAN Network Identifier), identifiant du r√©seau virtuel.
‚Üí Comparable √† un VLAN ID, mais au-dessus de UDP.
* `dstport 4789` : port standard VXLAN.
* `ens160` : interface physique du n≈ìud (underlay).
* `vxlan0` est branch√© sur `cni0`, qui relie les Pods locaux.

==== R√¥le de la FDB (Forwarding Database)

Le bridge `cni0` fonctionne en L2. Il ne conna√Æt que des MAC ‚Üí port.
Flannel installe des entr√©es dans la FDB pour associer un PodCIDR distant √† une IP de n≈ìud :

[source,bash]
----
bridge fdb add 00:00:00:00:00:00 dev vxlan0 dst 10.33.18.11
----

* `00:00:00:00:00:00` : wildcard, signifie "flooder tout trafic inconnu".
* `dst 10.33.18.11` : IP du n≈ìud distant (underlay).
* Donc : tout trafic destin√© √† des Pods inconnus ‚Üí encapsul√© et envoy√© vers `10.33.18.11` via `vxlan0`.

==== Routes install√©es

En parall√®le, Flannel ajoute une route pour chaque PodCIDR distant :

[source,bash]
----
ip route add 10.244.2.0/24 dev cni0
----

* Ici, `10.244.2.0/24` est le PodCIDR du n≈ìud2.
* Cela dit au kernel : *¬´ pour joindre 10.244.2.x, envoie via cni0 ¬ª*.

==== Flux ultra d√©taill√© et explication du vxlan

Pour comprends ce flux ultra d√©taill√©, il faut consulter la fiche sur le kernel kernel linux c√¥t√© r√©seau ou le maitriser.

Nous partons ici d'une requ√™te envoy√©e du pod1 du node1 vers le pod1 du node2. Nous d√©taillerons chaque √©tape, y compris tr√®s pr√©cis√©ment les √©tapes au niveau du kernel. Tout commence par un paquet, et donc un skb au niveau kernel.

La socket sur le pod1 cr√©√© un skb, elle l'envoie via `send_msg->tcp_transmit_skb`.
Le kernel lit le paquet via `ip_queue_xmit`, voit que l'ip de destination est `10.244.2.2`, elle est hors r√©seau.
Il fait un lookup fib, voit qu'il faut envoyer √† la gateway `10.244.1.1`.
Il sette donc le `skb->dst_entry` √† `10.244.1.1`. C'est le next hop. Il appelle `neigh_resolve_output` pour v√©rifier si l'adresse mac est connue.
Elle n'est pas connue, il fait une requ√™te arp (`who has 10.244.1.1`).
L'interface cni0 r√©ponds avec son adresse mac. Il enregistre l'adresse mac dans la table arp.

`skb_push` transforme le skb ip en skb trame ethernet en ajoutant notamment l'adresse mac de destination.
Le kernel appelle `dev_queue_xmit` qui place le skb dans la `tc_egress` puis le moment venu appelle `dev_hard_start_xmit`.
`dev_hard_start_xmit` voit que c'est une interface virtuelle, il consulte sa paire, la veth-eth0.
Il appelle `veth_xmit -> netif_rx` et appelle `netif_receive_skb`.

A ce stade nous avons :
- `dst_mac : mac_cni0`
- `skb->dev: veth-eth0`
- `dst_entry = cni0 ip`
- `ip paquet dst : 10.244.2.2`

`netif_receive_skb` appelle `netif_receive_skb_internal` qui v√©rifie si l'interface est un bridge.
C'est le cas, le kernel appelle `__netif_receive_sk_core` pour envoyer le skb vers la pile ip.

==== Passage dans la pile IP

Le skb entre dans la pile ip.
Dans l'ordre il commence par `PRE_ROUTING`, policy routing.
C'est ici que la d√©cision d'input ou forward est fait. L'adresse de destination n'est pas l'adresse de destination de la machine, c'est un forward.

Il y a lookup fib pour trouver le next hop qui est la vxlan0.
Le lookup regarde si via est sett√© sur la route :
- si il est sett√©, c'est notre prochain hop (`dst_entry = ipvia`)
- sinon il prend l'ip de destination du paquet (notre cas).

On a donc :
- `mac_unknown`
- `skb->dev: vxlan0`
- `dst_entry = 10.244.2.2 ip`
- `ip paquet dst : 10.244.2.2`

Ceci s'av√®re crucial dans le vxlan.

Les chaines FORWARD et POSTROUTING sont successivement appel√©es, puis `neigh_resolve_output`.

==== R√©solution ARP dans le VXLAN

La mac adresse n'est pas connue, donc le kernel fait une r√©solution arp, l'ip utilis√©e est alors `10.244.2.2` (`who has 10.244.2.2`).
La requ√™te arp est flood√©e, donc arrive sur vxlan0. vxlan0 l'encapsule dans un paquet udp √† destination de `10.33.18.11 (node2)` conform√©ment √† la d√©claration :

[source,bash]
----
add_fdb_on_namespace "node1" "vxlan0" "10.33.18.11"
----

==== Le paquet arrive sur node2

C‚Äôest un paquet UDP normal sur port 4789.
La pile IP de node2 le remet au driver VXLAN (socket UDP VXLAN est enregistr√© pour ce port).
üëâ Rien n‚Äôest encore dans vxlan0 tant que le d√©capsulage n‚Äôest pas fait.

==== vxlan_rcv() sur node2

Le driver vxlan lit le header VXLAN et v√©rifie :
- Que le port = 4789 (match socket vxlan).
- Que le VNI = celui de vxlan0.

üëâ C‚Äôest √ßa qui fait que le paquet est dispatch√© vers le bon vxlan device.

Le vxlan extrait le paquet arp, le flood √† son tour vers toutes les interfaces sauf celle o√π il l'a re√ßu (ens160).
Donc il transmet √† cni0, qui transmet √† eth0 pod2 qui r√©pond !

==== Retour vers node1

Tout ceci fait le chemin retour pour arriver vers la `neigh_resolve_output` o√π nous nous √©tions arr√™t√©s.
On a ainsi :
- `mac: pod1node2 (et √ßa change tout)`
- `skb->dev: vxlan0`
- `dst_entry = 10.244.2.2 ip`
- `ip paquet dst : 10.244.2.2`

Puis :
`skb_push -> dev_queue_xmit -> tc_egress -> dev_hard_start_xmit -> interface virtuelle -> pile ip de la vxlan0`

`vxlan0` encapsule, transmet √† ens160. Tout ceci arrive √† la vxlan, qui d√©capsule le paquet.
Il voit :
- `skb->dev: inconnue`
- `dst_entry = null ip`
- `ip paquet dst : 10.244.2.2`

Il repasse dans la pile IP ‚Üí fib ‚Üí route vers cni0 etc pour arriver sur pod2.

==== Conclusion

Voici comment fonctionne le vxlan.

=== Calico en mode BGP (sans tunnel) : du peering √† la FIB

Pour cette section nous stopperons la configuration en ligne de commande, elle n'est plus utile, les bases th√©oriques sont toutes pos√©es. L'objectif du BGP est de mettre √† jour la forwarding table des noeuds. Tout est d√©j√† pos√©. Voyons maintenant comment tout ceci fonctionne. Commen√ßons par quelques rappels sur kubernetes.

==== Introduction - Pr√©-requis kubernetes & allocation des ipds

===== Comment kubernetes g√®re le r√©seau du pod

- Kubernetes attribue un **PodCIDR** √† chaque n≈ìud du cluster.
- Ce PodCIDR est inscrit dans l‚Äôobjet Node (`.spec.podCIDR`).
- Exemple :
* `node1` ‚Üí `10.244.1.0/24`
* `node2` ‚Üí `10.244.2.0/24`

üëâ Kubernetes **ne g√®re pas directement l‚Äôallocation des IPs aux Pods**.
Il d√©l√®gue cette t√¢che au plugin CNI (Flannel, Calico, Cilium‚Ä¶).
Le CNI se charge ensuite d‚Äôattribuer une adresse IP issue de ce PodCIDR pour chaque Pod.

===== Flannel (mode simple, host-gw ou VXLAN)

- Flannel **pioche directement** dans le PodCIDR fourni par Kubernetes.
- Chaque Pod re√ßoit une IP issue de ce subnet.
- Le n≈ìud **annonce le PodCIDR complet** (ex: `/24`) aux autres n≈ìuds.
- Peu importe si seulement 2 Pods sont actifs ‚Üí tout le bloc est rout√©.

Exemple :
[source,bash]
----
# PodCIDR attribu√© par Kubernetes
node1 : 10.244.1.0/24
node2 : 10.244.2.0/24

# Flannel (VXLAN ou host-gw) annonce directement :
10.244.1.0/24 reachable via node1
10.244.2.0/24 reachable via node2
----

üëâ Simplicit√©, mais parfois sur-allocation des IPs.

===== Calico (mode BGP avec IPAM)

===== D√©finition IPAM

IPAM = *IP Address Management* (gestion des adresses IP).

Avec Calico :
- Le PodCIDR est **d√©coup√© en blocs IPAM plus petits** (par d√©faut `/26`, soit 64 adresses).
- Chaque n≈ìud r√©serve un ou plusieurs blocs IPAM en fonction de ses besoins.
- Ce sont **ces blocs pr√©cis** qui sont annonc√©s en BGP.

Exemple :
- Kubernetes attribue √† `node1` le PodCIDR `10.244.0.0/24`.
- Calico r√©serve pour `node1` le bloc `10.244.0.64/26`.
- `node1` annonce donc en BGP uniquement `10.244.0.64/26`.

üëâ R√©sultat :
- Allocation plus fine.
- Moins de gaspillage d‚Äôadresses.
- Possibilit√© d‚Äô√©tendre dynamiquement (nouveau bloc allou√© si besoin).

===== Pourquoi Calico fait √ßa ?

* Pour √©viter d‚Äôannoncer des plages vides.
* Pour avoir une granularit√© plus fine et optimiser l‚Äôusage des IPs.
* Pour permettre une gestion plus dynamique et scalable des adresses.

===== Que fait Calico si l‚ÄôIPAM se remplit ?

- Si le bloc `/26` d‚Äôun n≈ìud est plein ‚Üí Calico r√©serve **un autre bloc** du PodCIDR global.
- Ce bloc est √† son tour annonc√© en BGP.
- Exemple :
* Premier bloc : `10.244.0.64/26` (plein)
* Nouveau bloc : `10.244.0.128/26`
* Annonc√© par UPDATE ‚Üí `10.244.0.128/26`

Passons au BGP, nous y verrons une explication plus claire sur les updates.

==== Vue d‚Äôensemble et bases du BGP

Ceci n'est qu'un rappel, voir la fiche BGP pour plus d'explications.

Le protocole BGP repose sur la notion de *Syst√®me Autonome (AS)*.
Chaque AS est identifi√© par un num√©ro unique : **ASN (Autonomous System Number)**.

==== ASN publics

* Attribu√©s par l‚Äô**IANA** puis distribu√©s par les registres r√©gionaux (RIR) :
**RIPE**, **ARIN**, **APNIC**, **LACNIC**, **AFRINIC**.
* Utilis√©s pour les AS interconnect√©s √† Internet.
* Exemple :
** `AS16276` = OVH
** `AS3215` = Orange
** `AS15169` = Google

Ces ASN doivent √™tre **uniques globalement** sur Internet.

==== ASN priv√©s

Il existe aussi des plages r√©serv√©es pour un usage interne, non routable sur Internet :

* **16 bits** : `64512 ‚Äì 65534`
* **32 bits** : `4200000000 ‚Äì 4294967294`

Ces ASN sont utilis√©s dans :
* Les clusters Kubernetes (ex: Calico utilise `64512` par d√©faut).
* Les environnements internes d‚Äôentreprise.
* Les labos ou simulations.

==== Exemple

- Si je configure mon cluster Calico avec `ASN = 64512` :
Tous mes n≈ìuds font partie d‚Äôun m√™me AS priv√©, et √©changent leurs routes BGP uniquement entre eux.

==== R√¥le d‚Äôun routeur BGP

La seconde notion fondamentale du BGP est le **routeur**.
C‚Äôest lui qui √©tablit les sessions BGP, √©change les routes et alimente la table de routage (FIB).

Un routeur BGP a trois grandes responsabilit√©s :

. **√âtablir des sessions BGP**
* Communication en TCP sur le port `179` avec ses voisins explicitement configur√©s.
* √âchange de messages :
** `OPEN` (√©tablissement de session, annonce de l‚ÄôASN, timers, etc.)
*** Les routeurs √©changent notamment les timer **Hold Timer** (dur√©e maximale pendant laquelle un routeur peut rester sans nouvelles du voisin avant de consid√©rer la session morte), et **keepalive time**, fr√©quence d'envoie. Par d√©faut, selon la RFC 4271 : **Hold Timer** = 180s et **Keepalive Timer** = 60s. On r√©duit souvent ces timers sur calico pour d√©tecter plus rapidement les coupures (holdtime=30s et keepalive=10s)
** `KEEPALIVE` (maintien de la session, donc par d√©faut toutes les 60 secondes)
** `UPDATE` (annonces/retraits de routes)

. **Maintenir une RIB (Routing Information Base)**
* Base interne contenant toutes les routes apprises via BGP.
* Chaque entr√©e est associ√©e √† des attributs (AS_PATH, NEXT_HOP, LOCAL_PREF‚Ä¶).
* Application des r√®gles BGP pour choisir un **meilleur chemin** parmi les annonces disponibles.

. **Installer les routes dans la FIB (Forwarding Information Base)**
* La FIB correspond √† la table de routage effective du syst√®me (`ip route show`).
* C‚Äôest elle qui est consult√©e par le kernel (ou ASIC sur un vrai routeur) pour forwarder les paquets.
* BGP = plan de contr√¥le, Kernel = plan de donn√©es.

==== Les annonces BGP (Messages UPDATE)

Un message `UPDATE` permet :

* d‚Äô**annoncer** de nouveaux pr√©fixes (r√©seaux accessibles via ce routeur)
* ou de **retirer** (withdraw) des pr√©fixes devenus inaccessibles.

.Exemple : Node1 avec un bloc IPAM
----
Node1 poss√®de le bloc IPAM 10.244.1.0/24.

Il envoie √† ses voisins un UPDATE :
NLRI (Network Layer Reachability Info) = 10.244.1.0/24
Path attributes = { AS_PATH: 64512, NEXT_HOP: 10.33.18.10 }
----

Ses voisins stockent cela dans leur RIB :
*"Pour atteindre 10.244.1.0/24, passe par 10.33.18.10"*.

==== R√©sum√©

* **AS** : ensemble de r√©seaux sous une politique de routage unique.
* **Routeurs BGP** :
** √©tablissent des sessions point-√†-point avec des voisins
** √©changent des `UPDATE` (pr√©fixes + attributs)
** maintiennent une RIB et choisissent le meilleur chemin
** installent les routes retenues dans la FIB
* **Les annonces** portent sur des pr√©fixes IP (PodCIDR ou blocs IPAM en Kubernetes/Calico).
* **Les attributs** (AS_PATH, NEXT_HOP, LOCAL_PREF, MED) permettent de prendre la d√©cision de routage.

Dans un cluster Calico en **mode BGP sans encapsulation** :
- **Chaque n≈ìud** joue le r√¥le de **routeur BGP**.
- Tous les n≈ìuds appartiennent **au m√™me AS** (iBGP) par d√©faut (ex. `64512`).
- On appelle prefixe une notation CIDR qui d√©crit un bloc d'adresses ip (R√©seau + masque).
- Chaque n≈ìud **annonce** ( via des annonces de type UPDATE) les pr√©fixes dont il est propri√©taire (PodCIDR ou blocs IPAM) aux **voisins BGP**.
- Les d√©mons BGP **n‚Äôacheminent pas** les paquets : ils **apprennent/s√©lectionnent** des routes et les **installent** dans la FIB du noyau (Forwarding Information Base). Le **kernel** fait ensuite le **forwarding IP**.

==== Qui tourne, et qui configure quoi

- **Felix** : agent dataplane Calico. Programme le kernel (routes locales /32 des Pods via `cali*`, sysctls, policy), et publie/consomme l‚Äô√©tat r√©seau dans les CRD Calico (API K8s).
- **D√©mon BGP** (BIRD/GoBGP, embarqu√© dans `calico-node`) : √©tablit les **sessions BGP** et √©change les **routes**.
- **Confd** : g√©n√®re la **configuration BGP** (voisins, ASN, ce qu‚Äôon exporte) √† partir des CRD Calico :
- `BGPConfiguration` (ASN, full-mesh on/off, RR, etc.)
- `BGPPeer` (peers explicites ou RR)
- **Optionnel** : **Typha** (r√©duction de charge sur l‚ÄôAPI).

==== D√©couverte/peering des voisins (iBGP)

Deux approches principales c√¥t√© Calico :
- **Full-mesh iBGP** (par d√©faut) : chaque n≈ìud peer avec **tous** les autres n≈ìuds du cluster (O(N¬≤) sessions).
- **Route Reflector (RR)** : quelques n≈ìuds sont promoteÃÅs RR ; chaque n≈ìud peer **uniquement** avec 1‚ÄìN RR (scalable).

Tous partagent **le m√™me ASN** (ex. `64512`) ‚Üí c‚Äôest de **l‚ÄôiBGP**. L‚ÄôAS **n‚Äôidentifie pas un routeur** mais **un groupe** de routeurs.

==== √âtablissement d‚Äôune session BGP (iBGP ou eBGP)

*Pr√©-requis* : reachability IP entre adresses de peering (souvent **NodeIP underlay**).

*√âtablissement* :
1. **Connexion TCP** sur le port `179` vers l‚Äôadresse IP du voisin.
2. **√âchange `OPEN`** : version BGP, **ASN**, **BGP Identifier** (souvent une IP), **Hold Time**, **Optional Parameters** (capabilities : multiprotocol, route-refresh, 4-byte ASN, etc.).
3. **`KEEPALIVE` p√©riodiques** pour maintenir la session (ex: keepalive 30s, hold-time 90s).
4. En cas d‚Äôanomalie, **`NOTIFICATION`** et fermeture.

Une fois **`Established`**, les voisins peuvent s‚Äô√©changer des **`UPDATE`**.

==== Message `UPDATE` BGP (annonce/retrait) - Exemple d√©taill√©

Un `UPDATE` contient **trois blocs** :
- **Withdrawn Routes** : liste de pr√©fixes √† **retirer**.
- **Path Attributes** : m√©ta-infos pour les NLRI (ex. `ORIGIN`, `AS_PATH`, **`NEXT_HOP`**, `LOCAL_PREF`, `MED`, communaut√©s‚Ä¶).
- **NLRI (Network Layer Reachability Information)** : liste des **pr√©fixes annonc√©s**.

Dans Calico iBGP (exemples typiques) :
- **NLRI** : PodCIDR par n≈ìud **ou** blocs IPAM (ex. `/26`) attribu√©s au n≈ìud.
- **AS_PATH** : contient l‚ÄôASN du cluster (ex. `64512`) ‚Äî en iBGP, l‚Äô`AS_PATH` ne s‚Äôallonge pas au sein du m√™me AS.
- **NEXT_HOP** : **IP underlay du n≈ìud** qui annonce (ex. `10.33.18.11`). *C‚Äôest cet attribut qui dit concr√®tement ‚Äúvers QUEL n≈ìud envoyer‚Äù.*

.Exemple d‚Äôannonces (3 n≈ìuds, iBGP, ASN 64512)
[source,text]
----
# Node1 (NodeIP 10.33.18.10) ‚Üí voisins :
UPDATE
  NLRI:      10.244.1.0/24
  AS_PATH:   64512
  NEXT_HOP:  10.33.18.10

# Node2 (NodeIP 10.33.18.11) ‚Üí voisins :
UPDATE
  NLRI:      10.244.2.0/24
  AS_PATH:   64512
  NEXT_HOP:  10.33.18.11

# Node3 (NodeIP 10.33.18.12) ‚Üí voisins :
UPDATE
  NLRI:      10.244.3.0/24
  AS_PATH:   64512
  NEXT_HOP:  10.33.18.12
----

==== O√π vont ces annonces : RIB BGP ‚Üí d√©cision ‚Üí FIB kernel

La vie d‚Äôune route appris par BGP sur un n≈ìud *r√©cepteur* (ex. Node1 re√ßoit l‚ÄôUPDATE de Node2) :

1. **Adj-RIB-In (BGP)** : le d√©mon BGP stocke la route telle que re√ßue (ex. `10.244.2.0/24, NEXT_HOP 10.33.18.11`).
2. **Processus de s√©lection** (BGP Decision Process) :
- **LOCAL_PREF** (le plus haut pr√©f√©r√© ; interne au m√™me AS)
- **AS_PATH** (le plus court pr√©f√©r√©) ‚Äî trivial en iBGP Calico
- **ORIGIN** (`IGP` < `EGP` < `INCOMPLETE`)
- **MED**, eBGP/iBGP, e.t.c., jusqu‚Äôau tie-breaker (Router-ID‚Ä¶)
- *Dans Calico intra-cluster, il y a g√©n√©ralement un unique meilleur chemin par pr√©fixe.*
3. **Loc-RIB (BGP)** : stockage de la **meilleure** route.
4. **Injection dans la RIB du kernel** : le d√©mon BGP **programme le kernel** via **Netlink** (directement, ou via `zebra` pour FRR).
5. **FIB (kernel)** : la route devient **active** pour le forwarding.

.Exemple (sur Node1 apr√®s apprentissage de Node2/Node3)
[source,text]
----
# Routes locales Pods du Node1 (pos√©es par Felix)
10.244.1.0/24 dev cni0  proto kernel  scope link  src 10.244.1.1

# Routes BGP vers Pods distants (pos√©es par le d√©mon BGP)
10.244.2.0/24 via 10.33.18.11 dev ens160  proto bgp
10.244.3.0/24 via 10.33.18.12 dev ens160  proto bgp
----

Pour rappel pour refaire le pont avec ce que nous avons vu, **Ce que fait le noyau ensuite** (datapath) :
- Pour un paquet √† destination `10.244.2.13` :
1) **Lookup FIB** ‚Üí match `10.244.2.0/24 via 10.33.18.11 dev ens160`.
2) **Neighbour/ARP** sur `10.33.18.11` si n√©cessaire ‚Üí MAC du node2.
3) **Envoi L3 natif** vers node2 (pas d‚Äôencapsulation VXLAN).
4) Sur node2 : lookup FIB ‚Üí route **/32** locale du Pod via `cali*` ‚Üí livraison √† l‚Äôinterface veth du Pod.

==== Retrait/√©volution (withdraw) et stabilit√©

- Si un bloc IPAM n‚Äôest plus ‚Äúposs√©d√©‚Äù par un n≈ìud (plus de Pods, ou r√©allocation) :
- Le d√©mon BGP √©met un **`withdraw`** pour ce pr√©fixe.
- Les voisins retirent la route de leur **Loc-RIB**, puis de leur **FIB** kernel.

== Gestion des IPs non-n≈ìuds (Services, NodePort, ClusterIP) ‚Äì r√¥le de kube-proxy

Nous savons d√©sormais comment kubernetes pose les bases des r√©seaux associ√©s aux noeuds, nous avons pos√©s les diff√©rentes options et configurations.

Continuons d√©sormais vers les services.

=== Pr√© r√©quis

On consid√®re pour commencer que la cni est d√©ploy√©e, le r√©seau est multi pod. Autrement dit le pod1 du node1 peut joindre le pod2 du node2, peu importe la cni choisie.

Pour plus de r√©alisme on simule un serveur http sur le node2 pod1.

[source,bash]
----
ip netns exec node2-pod1 python3 -m http.server 8080
----

On souhaite d√©sormais exposer ce service, id√©alement on aimerait l'exposer sur un port du node2, disons le port 30080.

On souhaite donc exposer le service de la fa√ßon suivante:

[source,bash]
----
curl http://10.33.18.11:30080
----

Cela nous permettra de joindre le pod de l'ext√©rieur sans avoir √† d√©clarer le sous r√©seau dans notre fib.

=== kube-proxy en iptables

Ici une maitrise parfaite des iptables est n√©cessiares, et on conseillera d'ouvrir le deep dive sch√©ma sur le kernel pour plus de compr√©hensions.

Nous cr√©ons les helpers suivants :

[source,bash]
----
# $1 chain
# $2 table
function createChainOnTable() {
  iptables -t $2 -N $1 2>/dev/null || true
}

# $1 table
# $2 fromChain
# $3 fromPort
# $4 toChain
function forwardTCPPacketFromPortToChain() {
 iptables -t $1 -A $2 -p tcp --dport $3 -j $4
}

# $1 table
# $2 fromChain
# $3 toChain
function createJumpToChainFrom() {
   iptables -t $1 -C $2 -j $3 2>/dev/null ||
   iptables -t $1 -A $2 -j $3
}

----

[source,bash]
----
createChainOnTable "KUBE-NODEPORTS" "nat"
createChainOnTable "KUBE-SVC-NODEPORT" "nat"
createChainOnTable "KUBE-SEP-REMOTE" "nat"

createJumpToChainFrom "nat" "OUTPUT" "KUBE-NODEPORTS"
createJumpToChainFrom "nat" "PREROUTING" "KUBE-NODEPORTS"
----

[source,bash]
----
forwardTCPPacketFromPortToChain "nat" "KUBE-NODEPORTS" 30080 "KUBE-SVC-NODEPORT"
----

Ici on a donc tous les paquets √† destination du port 30080, qu'ils soit cr√©√©s en local (par un curl par ex) ou bien qu'ils proviennent de l'ext√©rieur, redirig√©s vers la chaine "KUBE-SVC-NODEPORT"

[source,bash]
----
createJumpToChainFrom "nat" "KUBE-SVC-NODEPORT" "KUBE-SEP-REMOTE"
----

[source,bash]
----
iptables -t nat -A KUBE-SEP-REMOTE -p tcp -j DNAT --to-destination 10.244.2.2:8080
----

C'est dans la commande ci-dessus que la magie intervient, on redirige tous les flux de la SEP-REMOTE vers la 10.244.2.2:8080.

Donc tous les paquets qui arrivent sur le port 30080, qu'ils soient envoy√©s en local, ou √† distance, on change la destination vers 10.244.2.2:8080. C'est exactement ce que l'on souhaite, on fait cela sur chaque noeud, si ce n'est pas sur le bon neoud, le cni fera le travail de redirection comme nous l'avons vu.

C'est ainsi que le kube-proxy fonctionne, c'est lui qui a la responsabilit√© de g√©rer les ips des services √† travers le cluster.

Il g√®re √©galement le masquerade en sortie des pods. Il fait comme ceci :

[source,bash]
----
# Cha√Æne KUBE-POSTROUTING (table nat) + hook POSTROUTING
createChainOnTable "KUBE-POSTROUTING" "nat"
createJumpToChainFrom "nat" "POSTROUTING" "KUBE-POSTROUTING"
----

Dans un premier temps, on ajoute l'ip du noeud par lequel est pass√© le traffic comme ip source lorsque la destination est un pod, et que la source n'en est pas un. Ainsi les pods connaitront l'ip du noeud qui a re√ßu le traffic.

[source,bash]
----
# SNAT (MASQUERADE) si la source n‚Äôest pas cluster et la destination est un Pod
iptables -t nat -A KUBE-POSTROUTING \
  ! -s 10.244.0.0/16 -d 10.244.0.0/16 \
  -j MASQUERADE
----

Le kube-proxy g√®re √©galement un autre cas, que l'on pas encore trait√© du node1 appelle un pod du node1 en passant par le nodeport. Pour le moment, si on fait cela, il n'y aura pas de sym√©trie au retour, pourquoi ?

Parce qu'√† l'aller ce sont les iptables qui routent le traffic vers le noeud de destination, la source est flaggu√©e de l'ip et du port du pod source. Mais au retour, le traffic passera par la cni0, pas par le noeud par lequel il est pass√© au chemin aller.

Cela s'appelle le hairpin problem. Ainsi on fait un masquerade lorsque le traffic fait du pod -> pod par un service.

[source,bash]
----
iptables -t nat -A KUBE-POSTROUTING \
  -s 10.244.0.0/16 -d 10.244.0.0/16 -o cni0 \
  -j MASQUERADE
----

Ainsi :

Donc :

* PodB croit que la requ√™te vient du bridge du n≈ìud, pas directement de PodA.
* PodB r√©pond √† 10.244.1.1 ‚Üí la r√©ponse revient dans le n≈ìud.
* Conntrack d√©fait le DNAT et renvoie correctement √† PodA.
üëâ Retour sym√©trique, probl√®me r√©solu.

=== Et les endpoint slices ?

Nous sommes donc capables √† l'aide des iptables de rediriger du traffic d'un port (ce que nous avons vu) vers un pod, ou bien de l'ip d'un service (clusterip que nous n'avons pas vu mais qui a la m√™me logique) vers un pod.

Le kube-proxy doit donc disposer de l'ip des services et de l'ip des pods. Les endpoint slices repr√©sentent ces ips, il s'agit de ips/port des backends. Donc ce qui est expos√©.

Kube-proxy watch ces endpointSlices, et reconstruit:

* en iptables : KUBE-SVC-‚Ä¶ (service) et autant de KUBE-SEP-‚Ä¶ (endpoints) avec DNAT,
* en IPVS : un Virtual Server (VIP:port) + Real Servers (pods IP:port).


=== Introduction √† IPVS (IP Virtual Server)

Linux int√®gre nativement un moteur de **load balancing L4** performant : **IPVS**.
Il est bas√© sur le module noyau `ip_vs` et fournit un √©quivalent d‚Äôun r√©partiteur de charge TCP/UDP, directement au niveau r√©seau.
C‚Äôest la brique qui permet √† Kubernetes de proposer un mode `--proxy-mode=ipvs` pour kube-proxy.

==== Principes

* Un **Virtual Server (VS)** = une IP:port virtuelle expos√©e (ex: `10.33.18.11:30080`).
* Un ou plusieurs **Real Servers (RS)** = backends r√©els (ex: `10.244.2.2:8080`, `10.244.2.3:8080`).
* IPVS choisit un backend pour chaque connexion entrante, conserve l‚Äôaffinit√© via une table de sessions, et r√©√©crit les paquets (DNAT/SNAT) si besoin.

==== Modes de fonctionnement

* **NAT** : le plus courant (DNAT vers le Pod, SNAT pour le retour).
* **DR (Direct Routing)** : la r√©ponse repart directement du backend au client (utile sur le m√™me L2).
* **TUN** : encapsulation IP-in-IP vers les backends (utile en inter-r√©seaux).

==== Algorithmes de r√©partition

IPVS fournit plusieurs strat√©gies natives :

* `rr` : round-robin (√©quilibrage simple).
* `lc` : least-connections (choix du backend le moins charg√©).
* `wrr` : weighted round-robin (pond√©ration).
* `sh` : source-hash (affinit√© client).
* `sed`, `dh`, `nq` : variantes avanc√©es pour affinit√© et performance.

==== Exemple pratique

Un Virtual Server `10.33.18.11:30080` peut r√©partir sur deux Pods :

[source,text]
----
VS:  10.33.18.11:30080 TCP
  -> RS: 10.244.2.2:8080  (weight 1)
  -> RS: 10.244.2.3:8080  (weight 1)
----

Avec l‚Äôalgorithme round-robin :
- la 1√®re connexion ira vers `10.244.2.2:8080`,
- la 2√®me vers `10.244.2.3:8080`,
- et ainsi de suite.

==== Pourquoi utiliser IPVS

* R√©partition **dans le noyau**, ultra-rapide.
* Gestion efficace de milliers de Services/Endpoints (scalabilit√© sup√©rieure √† iptables).
* Algorithmes vari√©s (affinit√©, pond√©ration).
* S‚Äôint√®gre √† Kubernetes via kube-proxy en mode IPVS.

üëâ En r√©sum√© : IPVS est un load balancer **L4 natif Linux**, simple √† configurer, tr√®s performant et parfaitement adapt√© √† Kubernetes ou √† des usages hors-cluster (VIP internes, labos, HA).

=== kube-proxy et l'IPVS

Le mode **IPVS** (IP Virtual Server) s‚Äôappuie sur le load balancer L4 du noyau Linux.
Contrairement au mode iptables (qui empile des DNAT), IPVS maintient une table
de Virtual Servers (VIP:port ‚Üî Real Servers) et fait le choix de backend
au fil de l‚Äôeau, directement dans le kernel, avec des algorithmes de LB.

==== Pourquoi IPVS ?

* Performances sup√©rieures quand il y a **beaucoup** de Services/Endpoints.
* Choix d‚Äôalgorithmes (rr, lc, dh, sh, sed, nq‚Ä¶).
* √âtat/compteurs d√©taill√©s, timeouts par protocole.
* Moins de *r√®gles mur de briques* ; kube-proxy ne programme plus des centaines de DNAT.

==== Pr√©-requis noyau / userspace

* Modules noyau (charg√©s automatiquement par kube-proxy si pr√©sents) :
** `ip_vs`, `ip_vs_rr`, `ip_vs_wrr`, `ip_vs_lc`, `ip_vs_sh`, `ip_vs_sed`, `ip_vs_dh`, `nf_conntrack`
* Binaire d‚Äôadmin (utile pour debug) : `ipvsadm`

===== Architecture (vue rapide)

* **kube-proxy** (mode `--proxy-mode=ipvs`) :
** watch `Service`, `EndpointSlice`
** cr√©e/maintient des **Virtual Servers** (les VIP/NodePort)
** attache des **Real Servers** (les IP:port des Pods)
** configure la **sessionAffinity**, les timeouts, l‚Äôalgo
* C√¥t√© datapath :
** IPVS fait le dispatch vers un Real Server
** iptables ne sert plus que de ‚Äúplomberie‚Äù minimale (rediriger VIP ‚Üí IPVS, masquerade/hairpin)

==== Plomberie iptables minimale
M√™me en IPVS, kube-proxy ajoute quelques r√®gles iptables :
* PREROUTING/OUTPUT nat ‚Üí saut vers cha√Ænes `KUBE-SERVICE`/`KUBE-NODEPORTS` **sp√©ciales IPVS**
* R√®gles **MASQUERADE** pour `externalTrafficPolicy=Cluster` et **hairpin** (comme en iptables)
* Marquage pour NodePort/ClusterIP afin que le trafic soit attrap√© par IPVS

==== Exemple ‚Äì 1 Service ‚Üí 2 Pods (sur le m√™me n≈ìud pour simplifier)

Supposons un Service `ClusterIP 10.96.0.42:80` et un NodePort `:30080`,
avec deux endpoints :
* `10.244.1.2:8080` (node1-pod1)
* `10.244.2.2:8080` (node2-pod1)

Apr√®s que kube-proxy ait converg√©, on peut inspecter IPVS :

[source,bash]
----
# Liste des Virtual Servers (VIP/NodeIP + port + algo)
ipvsadm -Ln

# D√©tail avec Real Servers
ipvsadm -Ln --stats --timeout
----

Sortie typique (sch√©matis√©e) :
[source,text]
----
TCP  10.96.0.42:80 rr
  -> 10.244.1.2:8080           Route   0     0          0
  -> 10.244.2.2:8080           Route   0     0          0

TCP  10.33.18.11:30080 rr      # NodePort sur le NodeIP
  -> 10.244.1.2:8080           Route   0     0          0
  -> 10.244.2.2:8080           Route   0     0          0
----

*Virtual Server* = l‚ÄôIP/port que tu contactes (ClusterIP, NodeIP:NodePort).
*Real Servers* = les endpoints Pod IP:port.

==== Algorithmes de load balancing

Configurer via annotations ou `Service.spec.sessionAffinity`/`kube-proxy` :
* `rr` (Round Robin)
* `wrr` (Weighted RR)
* `lc` (Least Connection)
* `sh` (Source Hash) ‚Äì utile pour affinit√© client sans cookie
* `sed`, `dh`, `nq` (variantes plus avanc√©es)

==== Session Affinity

* `sessionAffinity: ClientIP` c√¥t√© Service
* En IPVS, on utilise g√©n√©ralement `sh` (source-hash) avec un timeout d‚Äôaffinit√©
* kube-proxy programme IPVS pour coller au souhait du Service

==== NodePort & externalTrafficPolicy

* **NodePort** : IPVS cr√©e aussi un VS pour `NodeIP:nodePort`
* `externalTrafficPolicy: Cluster` (par d√©faut)
** SNAT/MASQ appliqu√© (src devient NodeIP) pour permettre le retour o√π que soit le Pod
* `externalTrafficPolicy: Local`
** le NodePort ne s√©lectionne que les endpoints **locaux** au n≈ìud
** pr√©serve l‚ÄôIP source du client (pas de SNAT) ‚Üí utile pour logs/ingress
** si aucun endpoint local, trafic dropp√© (sauf si tu actives HealthCheckNodePort c√¥t√© LB)

==== Hairpin (Pod ‚Üí Service/NodePort ‚Üí Pod sur m√™me n≈ìud)

M√™me cause, m√™me effet qu‚Äôen iptables : le **DNAT** (ici, redirection IPVS) casse la sym√©trie
si la r√©ponse part directement du PodB vers PodA.
kube-proxy pose donc, comme en mode iptables, des r√®gles **MASQUERADE hairpin** pour forcer
le retour via le n≈ìud, et laisser conntrack/IPVS recoller l‚Äôaller/retour.

R√®gle conceptuelle (illustration) :
[source,bash]
----
# Quand src et dst sont dans le PodCIDR local et que √ßa sort par cni0 ‚Üí MASQ
iptables -t nat -A KUBE-POSTROUTING \
  -s 10.244.0.0/16 -d 10.244.0.0/16 -o cni0 \
  -j MASQUERADE
----

==== Flux (r√©sum√©)

1. Paquet arrive sur `NodeIP:30080` (ou `ClusterIP:80`)
2. iptables minimal ‚Üí **attrape** et passe au **VS IPVS** appropri√©
3. IPVS choisit un Real Server (PodIP:port) selon l‚Äôalgo / affinit√©
4. Conntrack garde l‚Äô√©tat ; r√©ponses renvoy√©es au client (SNAT possible selon policy)
5. Cas hairpin : MASQ force le retour via le n≈ìud ‚Üí sym√©trie OK

==== D√©bogage pratique

[source,bash]
----
# √âtat IPVS
ipvsadm -Ln --stats --timeout

# Compteurs conntrack pour un tuple
conntrack -L | grep ':30080\|10\.96\.0\.42'

# R√®gles kube-proxy (plomberie)
iptables -t nat -S | grep -E 'KUBE-|MASQUERADE'
----

== Annexe : Cilium et eBPF - La r√©volution du datapath

Cilium est un CNI qui utilise la technologie **eBPF** (extended Berkeley Packet Filter) pour fournir networking, observability et s√©curit√© dans Kubernetes. Contrairement aux approches traditionnelles qui reposent sur iptables ou m√™me IPVS, Cilium d√©place la logique de traitement des paquets directement dans le noyau Linux via des programmes eBPF, offrant ainsi des performances et une flexibilit√© in√©dites.

=== Principes de base d'eBPF

* **eBPF** permet d'ex√©cuter des programmes sandbox√©s dans le noyau Linux sans en modifier le code source.
* Ces programmes s'attachent √† des **hooks** du noyau (ex: points r√©seau comme `XDP`, `tc`, `socket operations`).
* Ils op√®rent sur les paquets r√©seau et les connexions avec un overhead minimal.
* Les **maps eBPF** sont des structures de donn√©es partag√©es entre l'espace noyau et utilisateur pour stocker √©tats, r√®gles et m√©triques.

=== Ce que change Cilium concr√®tement

==== Remplacer kube-proxy (LoadBalancing des Services)

* **Plus de r√®gles iptables** : La gestion des Services (ClusterIP, NodePort) est impl√©ment√©e directement en eBPF.
* **D√©cision de LB ultra-rapide** : La lookup table Service ‚Üí Endpoints est une **hash table eBPF** consult√©e en temps constant.
* **Localit√©** : Le choix du backend et la r√©√©criture DNAT/SNAT ont lieu au point d'entr√©e le plus pr√©coce possible (e.g., XDP ou tc ingress).

==== Routage et politique r√©seau (NetworkPolicy)

* **Policies compil√©es en eBPF** : Les NetworkPolicy sont traduites en programmes eBPF attach√©s aux interfaces des Pods.
* **D√©cision au plus t√¥t** : Un paquet peut √™tre accept√© ou rejet√© d√®s son arriv√©e sur l'interface (tc ingress), sans traverser toute la stack.
* **Performances lin√©aires** : Le temps de traitement ne d√©pend pas du nombre de r√®gles (contrairement √† iptables et ses r√®gles lin√©aires).

==== Observability et s√©curit√©

* **Visibility native** : Cilium expose des m√©triques riches (L3-L7) via des maps eBPF, sans overhead significatif.
* **Security Identity** : Chaque Pod se voit attribuer une identity bas√©e sur les labels Kubernetes, utilis√©e pour appliquer les policies.

==== Exemple de flux pour un Service NodePort avec Cilium eBPF

. **Arriv√©e du paquet** : Un paquet arrive sur le port 30080 d'une interface du n≈ìud.
. **Hook XDP/tc** : Il est intercept√© par un programme eBPF attach√© en **XDP** (point le plus pr√©coce) ou **tc**.
. **Lookup Service** : Le programme consulte une map eBPF qui associe `IP:port` ‚Üí liste des backends (Pods).
. **Choix du backend** : Un algorithme de LB (e.g., random, maglev) s√©lectionne un endpoint.
. **R√©√©criture** : Le programme r√©√©crit les en-t√™tes (DNAT) et r√©injecte le paquet vers l'interface du Pod cible.
. **Retour** : Le paquet de r√©ponse est intercept√©, SNAT est appliqu√© via une autre lookup eBPF, et renvoy√© au client.

==== Avantages

* **Performance** : R√©duction de la latence et augmentation drastique du d√©bit (Mpps).
* **Scalabilit√©** : Gestion de dizaines de milliers de Services/Endpoints sans d√©gradation.
* **Fine-grained visibility** : Monitoring L7 natif (HTTP, gRPC) sans sidecar.
* **Security** : Policies r√©seau applicatives bas√©es sur des identit√©s Kubernetes.

==== Exemple de programme ebf de routing

[source,c]
----
#include <linux/bpf.h>
#include <linux/if_ether.h>
#include <linux/ip.h>
#include <linux/tcp.h>
#include <linux/in.h>
#include <bpf/bpf_helpers.h>
#include <bpf/bpf_endian.h>

// Map eBPF pour stocker les r√®gles de routage (Pod IP -> Node IP)
struct {
    __uint(type, BPF_MAP_TYPE_LPM_TRIE);
    __uint(max_entries, 1024);
    __type(key, struct bpf_lpm_trie_key);
    __type(value, __u32);
    __uint(map_flags, BPF_F_NO_PREALLOC);
} pod_to_node_map SEC(".maps");

// Map eBPF pour suivre les connexions
struct {
    __uint(type, BPF_MAP_TYPE_LRU_HASH);
    __uint(max_entries, 65536);
    __type(key, struct tuple);
    __type(value, struct connection_info);
} conntrack_map SEC(".maps");

// Structures de donn√©es
struct tuple {
    __u32 src_ip;
    __u32 dst_ip;
    __u16 src_port;
    __u16 dst_port;
    __u8 protocol;
};

struct connection_info {
    __u32 node_ip;
    __u16 flags;
};

SEC("xdp")
int route_pod_traffic(struct xdp_md *ctx) {
    void *data_end = (void *)(long)ctx->data_end;
    void *data = (void *)(long)ctx->data;

    struct ethhdr *eth = data;
    if (data + sizeof(*eth) > data_end)
        return XDP_PASS;

    // Seulement traiter le trafic IP
    if (eth->h_proto != bpf_htons(ETH_P_IP))
        return XDP_PASS;

    struct iphdr *iph = data + sizeof(*eth);
    if (data + sizeof(*eth) + sizeof(*iph) > data_end)
        return XDP_PASS;

    // V√©rifier si c'est du trafic entre pods
    if (is_pod_traffic(iph->saddr, iph->daddr)) {
        // Recherche dans la table de routage eBPF
        struct bpf_lpm_trie_key key = {
            .prefixlen = 32,
            .data = {iph->daddr}
        };

        __u32 *node_ip = bpf_map_lookup_elem(&pod_to_node_map, &key);
        if (node_ip) {
            // R√©√©crire l'en-t√™te Ethernet pour router vers le bon n≈ìud
            if (rewrite_eth_header(eth, *node_ip)) {
                // Mettre √† jour la table de connexion
                update_conntrack(iph, *node_ip);
                return XDP_TX;
            }
        }
    }

    return XDP_PASS;
}

// Fonction helper pour v√©rifier le trafic entre pods
static __always_inline bool is_pod_traffic(__u32 src_ip, __u32 dst_ip) {
    // V√©rifier si les IPs font partie du CIDR des pods
    // (impl√©mentation simplifi√©e)
    return (src_ip & 0xFFFF0000) == 0xC0A80000 && // 192.168.0.0/16
           (dst_ip & 0xFFFF0000) == 0xC0A80000;
}

// Fonction helper pour r√©√©crire l'en-t√™te Ethernet
static __always_inline bool rewrite_eth_header(struct ethhdr *eth, __u32 node_ip) {
    // R√©√©crire l'adresse MAC destination bas√©e sur l'IP du n≈ìud
    // (impl√©mentation simplifi√©e)
    __u8 *mac = get_node_mac(node_ip);
    if (!mac) return false;

    for (int i = 0; i < ETH_ALEN; i++)
        eth->h_dest[i] = mac[i];

    return true;
}

// Fonction helper pour mettre √† jour le suivi de connexion
static __always_inline void update_conntrack(struct iphdr *iph, __u32 node_ip) {
    struct tuple key = {0};
    struct connection_info info = {0};

    key.src_ip = iph->saddr;
    key.dst_ip = iph->daddr;
    key.protocol = iph->protocol;

    // Pour TCP/UDP, extraire les ports (simplifi√©)
    if (iph->protocol == IPPROTO_TCP || iph->protocol == IPPROTO_UDP) {
        struct tcphdr *tcph = (struct tcphdr *)(iph + 1);
        key.src_port = tcph->source;
        key.dst_port = tcph->dest;
    }

    info.node_ip = node_ip;
    info.flags = 0;

    bpf_map_update_elem(&conntrack_map, &key, &info, BPF_ANY);
}

char _license[] SEC("license") = "GPL";
----

==== Explication du programme eBPF

Ce programme eBPF illustre comment Cilium peut router le trafic entre pods sur diff√©rents n≈ìuds:

* D√©tection du trafic entre pods: La fonction is_pod_traffic v√©rifie si le trafic provient et est destin√© √† des pods (dans le CIDR 192.168.0.0/16).
* Table de routage eBPF: La map pod_to_node_map associe les IPs de pods aux IPs de n≈ìuds, permettant une recherche rapide de la destination.
* R√©√©criture des en-t√™tes: La fonction rewrite_eth_header modifie l'adresse MAC de destination pour diriger le paquet vers le n≈ìud appropri√©.
* Suivi des connexions: La map conntrack_map garde une trace des connexions √©tablies pour g√©rer correctement le trafic de retour.
* D√©cision de routage: Le programme prend la d√©cision de routage au niveau XDP (eXpress Data Path), le point d'entr√©e le plus pr√©coce dans la stack r√©seau.

==== Avantages de cette approche

* Performances: Le routage s'effectue dans le noyau, sans avoir besoin de remonter les paquets √† l'espace utilisateur.
* Flexibilit√©: Les r√®gles de routage peuvent √™tre mises √† jour dynamiquement via les maps eBPF.
* Visibilit√©: Le programme peut collecter des m√©triques d√©taill√©es sur le trafic r√©seau.
* Int√©gration avec Kubernetes: Cilium utilise l'API Kubernetes pour d√©couvrir les pods et les n≈ìuds, et met √† jour les maps eBPF en cons√©quence.

==== En r√©sum√©

Cilium et eBPF repr√©sentent l'√©volution la plus significative dans le networking Kubernetes depuis l'apparition des CNI. Ils permettent de d√©passer les limitations des m√©canismes traditionnels (iptables, VXLAN) en offrant un datapath programmable, haute performance et riche en fonctionnalit√©s, tout en simplifiant l'op√©ration.